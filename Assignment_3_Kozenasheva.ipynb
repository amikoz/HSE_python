{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "1. Implement linear regression model for multiclass classification using pytorch.\n",
    "2. Implement multinomial and one-vs-rest variants on multiclass classification.\n",
    "3. Implement L2 relularization for your model.\n",
    "4. Test your model on 20newsgroups dataset. Your baseline is accuracy=0.75.\n",
    "5. How can we justify using accuracy score for this problem?\n",
    "6. What is acuraccy score for random answer for this problem?\n",
    "\n",
    "Follow #TODO in the code below.\n",
    "Feel free to add additional regularizers to your model.\n",
    "Remember, that SGD convergence is slower that lbfgs from scikit-learn. Manage your time.\n",
    "\n",
    "Usefull links:\n",
    "https://pytorch.org/\n",
    "https://gluon.mxnet.io/chapter06_optimization/gd-sgd-scratch.html\n",
    "(bonus) http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tt\n",
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemms = SnowballStemmer('english')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tok = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# retrieve dataset\n",
    "data = fetch_20newsgroups()\n",
    "\n",
    "\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "#TODO some feature engineering\n",
    "# If you want to use some sparse feature vectors, pay attention to feature size.\n",
    "# While your feature matrix can be sparse, weight tensor in the model is always dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[0-9]', '', text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = text.strip()\n",
    "    words = [stemms.stem(word) for word in tok.tokenize(text)]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from lerxst wam umd edu where s my thing subject what car is this nntp post host rac wam umd edu organ univers of maryland colleg park line i was wonder if anyon out there could enlighten me on this car i saw the other day it was a door sport car look to be from the late s earli s it was call a bricklin the door were realli small in addit the front bumper was separ from the rest of the bodi this is all i know if anyon can tellm a model name engin spec year of product where this car is made histori or whatev info you have on this funki look car pleas e mail thank il brought to you by your neighborhood lerxst'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e588990bc043a88fbe3a5a0b08710c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [cleaning(i) for i in tqdm_notebook(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, stop_words='english', ngram_range=(1,2))\n",
    "X = tfidf.fit_transform(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 62019), (11314,))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    All neural networks in pytorch are descendants of nn.Module class\n",
    "    As you remember, Logistic regression is just a 1-layer neural network\n",
    "    #TODO implement multinomial logistic regression\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d, k):\n",
    "        \"\"\"\n",
    "        In the constructor we define model weights and layers\n",
    "        d: feature size\n",
    "        k: number of classes\n",
    "        \"\"\"\n",
    "        super(LogisticRegressionNN, self).__init__()\n",
    "        \n",
    "        # TODO create tensor of weights and tensor of biases\n",
    "        # initialize tensors from N(0,1) using np.random.rand\n",
    "        # W has shape (d,k)\n",
    "        # b has shape (d,)\n",
    "        # set requires_grad=True for tensors, so they can be learned during training\n",
    "        self.W = tt.tensor(np.random.rand(d, k), requires_grad=True, dtype=tt.float32)\n",
    "        self.b = tt.tensor(np.random.rand(k,), requires_grad=True, dtype=tt.float32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In this method we implement connections between neural network weights\n",
    "        x: batch feature matrix\n",
    "        returns: probability logits\n",
    "        \"\"\"\n",
    "        # TODO implement linear model without softmax\n",
    "        result = tt.matmul(x.double(), self.W.double()).add(self.b.double())\n",
    "        return result\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        learnable model parameters\n",
    "        \"\"\"\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    \n",
    "class LogisticRegressionEstimator(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Logistic Regression estimator coping interface from scikit-learn\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, n_epochs, batch_size, alpha=1, multi_class='multinomial', verbose=False):\n",
    "        \"\"\"\n",
    "        learning_rate: SGD learning rate\n",
    "        n_epochs: number of epochs\n",
    "        batch_size: size of mini-batch\n",
    "        alpha:  regularizer coef\n",
    "        multi_class: ['multinomial', 'ovr']\n",
    "        verbose:\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.alpha = alpha\n",
    "        self.multi_class = multi_class\n",
    "        self.verbose = verbose\n",
    "        self.model_nn = None\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def _train_nn(self, model, X, y):\n",
    "        \"\"\"\n",
    "        Train neural network\n",
    "        model: neural network module\n",
    "        X: - feature matrix\n",
    "        y: - target values\n",
    "        \"\"\"\n",
    "        \n",
    "        # criterion to minimize\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # optimization algorithm\n",
    "        optimizer = tt.optim.SGD(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        #TODO calculate number of batches, round to the ceil\n",
    "        n_batches = int(np.ceil(X.shape[0] / self.batch_size)) \n",
    "\n",
    "        if self.verbose:\n",
    "            # nice progress bar\n",
    "            t_epochs = tqdm_notebook(range(self.n_epochs), desc='epochs', leave=True)\n",
    "        else:\n",
    "            t_epochs = range(self.n_epochs)\n",
    "\n",
    "        # iterate over epochs\n",
    "        for epoch in t_epochs:\n",
    "\n",
    "            # TODO make random permutation over indices, use np.random.choice\n",
    "            indices = np.random.choice(list(range(X.shape[0])), size=X.shape[0])\n",
    "            \n",
    "            epoch_average_loss = 0\n",
    "\n",
    "            # iterate over mini-batches\n",
    "            for j in range(n_batches):\n",
    "\n",
    "                batch_idx = indices[j: j + self.batch_size]\n",
    "\n",
    "                # we have to wrap data into tensors before feed them to neural network\n",
    "                #TODO: batch feature float tensor. use tt.from_numpy\n",
    "                batch_x = tt.from_numpy(X[batch_idx].toarray())\n",
    "                #TODO batch target long tensor. use tt.from_numpy\n",
    "                batch_y = tt.from_numpy(y[batch_idx]).long()\n",
    "\n",
    "                # reset gradients for the new iteration\n",
    "                optimizer.zero_grad()\n",
    "                # get predictions\n",
    "                pred = model.forward(batch_x)\n",
    "                \n",
    "                # cross-entropy loss\n",
    "                loss = criterion(pred, batch_y)\n",
    "                #TODO: add regularizer on weights\n",
    "                loss += self.alpha/2 * tt.norm(model.W.double())**2\n",
    "\n",
    "                # calculate gradients\n",
    "                loss.backward()\n",
    "                # make optimization step\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_average_loss += loss.data.detach().item()\n",
    "\n",
    "            # average loss for epoch\n",
    "            epoch_average_loss /= n_batches\n",
    "            if self.verbose:\n",
    "                t_epochs.set_postfix(loss='%.3f' % epoch_average_loss)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: feature matrix\n",
    "        y: target values\n",
    "        \"\"\"\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        \n",
    "        # binary classification\n",
    "        if self.n_classes_ == 2:\n",
    "            self.model_nn = LogisticRegressionNN(n_features, 2)\n",
    "            self._train_nn(self.model_nn, X, y)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.multi_class == 'multinomial':\n",
    "                # TODO: multinomial classification\n",
    "                self.model_nn = LogisticRegressionNN(n_features, self.n_classes_)\n",
    "                self._train_nn(self.model_nn, X, y)\n",
    "                \n",
    "            # ovr classification\n",
    "            elif self.multi_class == 'ovr':\n",
    "                \n",
    "                if self.verbose:\n",
    "                    t_ovr = tqdm_notebook(range(self.n_classes_), desc='ovr')\n",
    "                else:\n",
    "                    t_ovr = range(self.n_classes_)\n",
    "                \n",
    "                # TODO: one-vs-rest classification\n",
    "                for key, i in enumerate(t_ovr):\n",
    "                    self.model_nn = LogisticRegressionNN(n_features, 2)\n",
    "                    y_new = np.array([1 if item == key else 0 for item in y])\n",
    "                    self._train_nn(self.model_nn, X, y_new)\n",
    "        return self\n",
    "                    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        if sparse.issparse(X):\n",
    "            # create sparse tensor\n",
    "            X = X.tocoo()\n",
    "            ii = tt.LongTensor([X.row, X.col])\n",
    "            X = tt.sparse.FloatTensor(ii, tt.from_numpy(X.data).float(), X.shape)\n",
    "        else:\n",
    "            # create dense tensor\n",
    "            X = tt.from_numpy(X).float()\n",
    "            \n",
    "        \n",
    "        if self.n_classes_ == 2:\n",
    "            pred = self.model_nn.forward(X)\n",
    "            pred = tt.softmax(pred, dim=-1)\n",
    "            pred = pred.detach().numpy()\n",
    "            return pred\n",
    "            \n",
    "        else:\n",
    "            if self.multi_class == 'multinomial':\n",
    "                # TODO return class probabilities\n",
    "                pred = self.model_nn.forward(X)\n",
    "                pred = tt.softmax(pred, dim=-1)\n",
    "                pred = pred.detach().numpy()\n",
    "                return pred\n",
    "                \n",
    "            elif self.multi_class == 'ovr':\n",
    "                # TODO return class probabilities\n",
    "                # remember to normalize probabities from different binary classification models, so they sum up to 1.0\n",
    "                pred = []\n",
    "                \n",
    "                for est in self.models:\n",
    "                    item = tt.sigmoid(est.forward(X))\n",
    "                    pred.append(item.detach().numpy()[:, 1])\n",
    "                \n",
    "                return normalize(np.array(pred).T)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return proba.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select hyperparams to obtain good quality in reasonable time\n",
    "\n",
    "est = LogisticRegressionEstimator(...)\n",
    "\n",
    "est.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7532, 62019), (7532,))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch test subset\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "X_test = tfidf.transform(test_data['data'])\n",
    "y_test = test_data['target']\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подбор с помощью GridSearchCV (работал оооочень долго и поэтому остановила)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select hyperparams to obtain good quality in reasonable time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate=1,\n",
    "    n_epochs=10,\n",
    "    batch_size=100,\n",
    "    alpha=1,\n",
    "    multi_class='multinomial',\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    est,\n",
    "    {'learning_rate': [1, 5, 10],\n",
    "     'alpha': [1e-5, 1e-3, 0.1, 1, 10],\n",
    "     'batch_size': [32, 64, 128]\n",
    "    },\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gscv = gscv.fit(X, y)\n",
    "print('Best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подбор вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 18s, sys: 1min 12s, total: 4min 31s\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 5,\n",
    "    n_epochs = 20,\n",
    "    batch_size = 32,\n",
    "    alpha = 1e-05,\n",
    "    multi_class = 'multinomial',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6485661178969729\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 26s, sys: 1min 24s, total: 7min 50s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 5,\n",
    "    n_epochs = 50,\n",
    "    batch_size = 64,\n",
    "    alpha = 1e-05,\n",
    "    multi_class = 'multinomial',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7033988316516198\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 8s, sys: 57 s, total: 7min 5s\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 5,\n",
    "    n_epochs = 50,\n",
    "    batch_size = 128,\n",
    "    alpha = 1e-05,\n",
    "    multi_class = 'multinomial',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7047265002655337\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 15s, sys: 1min 19s, total: 7min 35s\n",
      "Wall time: 5min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 5,\n",
    "    n_epochs = 50,\n",
    "    batch_size = 64,\n",
    "    alpha = 0,\n",
    "    multi_class = 'multinomial',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6911842804036112\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"из крайности в крайность\" попробовала с совсем другими параметрами и все равно не удалось побить бейзлайн "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 35min 7s, sys: 45min 4s, total: 3h 20min 12s\n",
      "Wall time: 1h 21min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 1,\n",
    "    n_epochs = 1000,\n",
    "    batch_size = 1000,\n",
    "    alpha = 0.001,\n",
    "    multi_class = 'multinomial',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6614445034519384\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 49s, sys: 4min 38s, total: 18min 28s\n",
      "Wall time: 8min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 10,\n",
    "    n_epochs = 100,\n",
    "    batch_size = 500,\n",
    "    alpha = 0.001,\n",
    "    multi_class = 'multinomial',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.5845724907063197\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 9s, sys: 4min 20s, total: 19min 29s\n",
      "Wall time: 7min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 10,\n",
    "    n_epochs = 100,\n",
    "    batch_size = 1000,\n",
    "    alpha = 0.001,\n",
    "    multi_class = 'multinomial',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.5860329261816251\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 35.4 s, total: 3min 14s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 5,\n",
    "    n_epochs = 20,\n",
    "    batch_size = 64,\n",
    "    alpha = 1e-05,\n",
    "    multi_class = 'multinomial',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6594530005310675\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ИТОГ multinomial: acc 0.7047265002655337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ovr попробуем с теми же параметрами, что дали наивысший acc multinominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 6min 35s, sys: 14min 44s, total: 1h 21min 20s\n",
      "Wall time: 43min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(\n",
    "    learning_rate = 5,\n",
    "    n_epochs = 50,\n",
    "    batch_size = 128,\n",
    "    alpha = 1e-05,\n",
    "    multi_class = 'ovr',\n",
    "    verbose = False\n",
    ")\n",
    "est.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При выдаче acc выдавал ошибку, я ее исправила, но тк работает почти полтора часа, не успею заново пересчитать("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we justify using accuracy score for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как у нас сбалансированы классы: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 480,\n",
       "         1: 584,\n",
       "         2: 591,\n",
       "         3: 590,\n",
       "         4: 578,\n",
       "         5: 593,\n",
       "         6: 585,\n",
       "         7: 594,\n",
       "         8: 598,\n",
       "         9: 597,\n",
       "         10: 600,\n",
       "         11: 595,\n",
       "         12: 591,\n",
       "         13: 594,\n",
       "         14: 593,\n",
       "         15: 599,\n",
       "         16: 546,\n",
       "         17: 564,\n",
       "         18: 465,\n",
       "         19: 377})"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разброс в значениях небольщой, можем сказать, что классы сбалансированы, значит accuracy нам подходит"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is acuraccy score for random answer for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы имеем 20 классов, следовательно вероятность получить верный ответ на 1 объекте 1/20 или 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
